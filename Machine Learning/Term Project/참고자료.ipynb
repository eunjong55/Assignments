{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"day5-1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMy4i1n2f0HoVnLw2kRu8sM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VhD8xy5neBFH"},"source":["# 9. Deep Learning with PyTorch\n","\n","In this session, we will practice building, training, and predicting with deep neural networks using the PyTorch library.\n","\n","Before we get started, import below listed libraries/packages."]},{"cell_type":"code","metadata":{"id":"JShvD6cLb4wG"},"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jTtwOq3ue8xC"},"source":["## 9.1. Getting Started with PyTorch\n","\n","First off, let us look around the key features that PyTorch offers.\n","\n","(9.1. code obtained from: https://github.com/yunjey/pytorch-tutorial) \n"]},{"cell_type":"markdown","metadata":{"id":"C2WPYneAw39r"},"source":["### 9.1.1. Autograd\n","\n","Autograd (automatic differentiation, AD) computes the derivatives from the network automatically."]},{"cell_type":"code","metadata":{"id":"O7RJbx93w2IA"},"source":["# Declare \"tensor\"-type variable, and create a \"computation graph\"\n","x = torch.tensor(1., requires_grad=True)\n","w = torch.tensor(2., requires_grad=True)\n","b = torch.tensor(3., requires_grad=True)\n","\n","y = w * x + b    # y = 2 * x + 3\n","\n","# The backward() function obtains the derivatives from the computation graph\n","y.backward()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TVS_qpLLx1Fa"},"source":["Let us check the results."]},{"cell_type":"code","metadata":{"id":"g0wmqijUx4_I"},"source":["print(x.grad)    # x.grad = 2 \n","print(w.grad)    # w.grad = 1 \n","print(b.grad)    # b.grad = 1 \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Urnvm6JiyFBI"},"source":["The following code snippet is a snapshot showing how the autograd feature is used in practice. (You do not need to execute the snippet at this moment.)"]},{"cell_type":"code","metadata":{"id":"MZKKY_ffyDj-"},"source":["# Create two matrices with random values. x is a 10x3 matrix; y is a 10x2 matrix.\n","x = torch.randn(10, 3)\n","y = torch.randn(10, 2)\n","\n","# Instantiating a linear regression model that takes x as input and y as output.\n","# (Using the neural network jargon, below is creating a fully connected layer.)\n","linear = nn.Linear(3, 2)\n","print ('w: ', linear.weight)\n","print ('b: ', linear.bias)\n","\n","# Declare the loss function and optimizer for training.\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n","\n","# Forward pass\n","pred = linear(x)\n","\n","# Compute loss\n","loss = criterion(pred, y)\n","print('** loss before 1 step optimization: ', loss.item())\n","\n","# Backward pass\n","loss.backward()\n","\n","# Print out the derivatives\n","print ('dL/dw: ', linear.weight.grad) \n","print ('dL/db: ', linear.bias.grad)\n","\n","# Run the first iteration of the gradient descent process.\n","optimizer.step()\n","# You can also perform gradient descent at the low level\n","# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n","# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n","\n","# Printing out the updated loss (output from the loss function) after the first iteration of gradient descent\n","pred = linear(x)\n","loss = criterion(pred, y)\n","print('** loss after 1 step optimization: ', loss.item())\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1iJ6Q9rlfiAn"},"source":["### 9.1.2. Loading data from `numpy`\n","\n","By using function `torch.from_numpy()`, one can easily convert a `ndarray`-type variable (a multi-dimensional matrix defined in `numpy`) into a torch specific data type (and *vice versa*)."]},{"cell_type":"code","metadata":{"id":"ltDxc25jf-7v"},"source":["import numpy as np\n","\n","# Create a numpy array\n","x = np.array([[1, 2], [3, 4]])\n","\n","# Convert: numpy array -> torch tensor\n","y = torch.from_numpy(x)\n","\n","# Convert: torch tensor -> numpy array\n","z = y.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ATMMriGPgE7G"},"source":["### 9.1.3. DataLoader\n","\n","`DataLoader` lets the user easily build a PyTorch-specific dataset for training and prediction. \n","\n","The next example shows how one can load the CIFAR-10 dataset using `DataLoader`. (Like `sklearn.datasets`, `torchvision.datasets` includes a few widely used datasets, such as MNIST and CIFAR-10.)"]},{"cell_type":"code","metadata":{"id":"rqK9xMXdgyK3"},"source":["# Download and load the CIFAR-10 dataset in the memory\n","train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n","                                             train=True, \n","                                             transform=transforms.ToTensor(),\n","                                             download=True)\n","\n","# Accessing the first instance of the dataset\n","image, label = train_dataset[0]\n","print (image.size())\n","print (label)\n","\n","# Instantiating a DataLoader using the loaded dataset\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=64, \n","                                           shuffle=True)\n","\n","# Code skeleton for training, with the above created DataLoader object\n","for images, labels in train_loader:\n","    # ----------------------------------------------\n","    # -- Your training code should be placed here --\n","    # ----------------------------------------------\n","    pass\n","\n","# One can compose mini-batches using Iterator\n","# data_iter = iter(train_loader)\n","# images, labels = data_iter.next()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6hWuL3TAherZ"},"source":["The next code skeleton shows how to create an object for custom (user supplied) data. With a complete `CustomDataset` class, one can instantiate a dataset object that works with the `DataLoader` class, which is explained above."]},{"cell_type":"code","metadata":{"id":"lKQj1umLhzAA"},"source":["# With a complete `CustomDataset` class, one can instantiate \n","# a dataset object that works with the `DataLoader` class\n","# (Before you comlete the sections marked with 'TODO', this code snippet does not run)\n","\n","# You should build your custom dataset as below\n","class CustomDataset(torch.utils.data.Dataset):\n","  def __init__(self):\n","    # TODO\n","    # 1. Initialize file paths or a list of file names\n","    pass\n","  def __getitem__(self, index):\n","    # TODO\n","    # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open)\n","    # 2. Preprocess the data (e.g. torchvision.Transform)\n","    # 3. Return a data pair (e.g. image and label)\n","    \n","    pass\n","  def __len__(self):\n","    # You should change 0 to the total size of your dataset\n","    return 0 \n","\n","# You can then use the prebuilt data loader\n","custom_dataset = CustomDataset()\n","train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n","                                           batch_size=64, \n","                                           shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eS6ztXhZiuCS"},"source":["### 9.1.4. Loading a Pretrained Model\n","\n","One can load a pretrained model that has been written and trained using PyTorch. The next code snippet shows how to bring in a pretrained ResNet-18 model (an image classifier)."]},{"cell_type":"code","metadata":{"id":"KskQhv0xjGwS"},"source":["# Download and load a pretrained ResNet-18 model\n","resnet = torchvision.models.resnet18(pretrained=True)\n","\n","\n","# One may train more (finetune) a pretrained model\n","# The next code shows how to train the last layer of the pretrained ResNet-18 model\n","for param in resnet.parameters():\n","    param.requires_grad = False\n","resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example\n","\n","\n","# By taking the forward pass of the model, the pretrained model can be applied to a prediction task\n","images = torch.randn(64, 3, 224, 224)\n","outputs = resnet(images)\n","print (outputs.size())     # (64, 100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eAEvk_IFjF-t"},"source":["## 9.2. Logistic Regression (in the PyTorch way!)\n","\n","This section shows a re-implementation of logistic regression using PyTorch, which is distinct from that of Scikit-learn or Numpy."]},{"cell_type":"markdown","metadata":{"id":"n1mO8UZTkueu"},"source":["We will load the MNIST dataset for this tutorial. The MNIST dataset consists of hand-written digits 0-9. \n","\n","![Image is not found](https://miro.medium.com/max/530/1*VAjYygFUinnygIx9eVCrQQ.png)"]},{"cell_type":"code","metadata":{"id":"3zTduECVk_1V"},"source":["input_size = 784\n","num_classes = 10\n","batch_size = 100\n","\n","# MNIST dataset \n","train_dataset = torchvision.datasets.MNIST(root='../../data', \n","                                           train=True, \n","                                           transform=transforms.ToTensor(),  \n","                                           download=True)\n","\n","test_dataset = torchvision.datasets.MNIST(root='../../data', \n","                                          train=False, \n","                                          transform=transforms.ToTensor())\n","\n","# Data loader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n","                                          batch_size=batch_size, \n","                                          shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m-GInpCQlCC5"},"source":["Next snippet defines a logistic regression model using PyTorch."]},{"cell_type":"code","metadata":{"id":"49vfbeRUlHqX"},"source":["# Hyper-parameter setting\n","learning_rate = 0.001\n","\n","# Model definition: building a network that consists of one linear layer\n","model = nn.Linear(input_size, num_classes)\n","\n","# Training-parameter setting: specifying the loss function(i.e., objective function) and optimizer for training\n","criterion = nn.CrossEntropyLoss()  # nn.CrossEntropyLoss() computes softmax internally\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"26vwz6JElZao"},"source":["Next code describes how the above model is trained and used for prediction."]},{"cell_type":"code","metadata":{"id":"m-NW-tpIlmlu"},"source":["# Train the model\n","def train_logreg(train_loader, num_epochs):\n","  total_step = len(train_loader)\n","  for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","      # Reshape images to (batch_size, input_size)\n","      images = images.reshape(-1, 28*28)\n","      \n","      # Forward pass\n","      outputs = model(images)\n","      loss = criterion(outputs, labels)\n","      \n","      # Backward and optimize\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      \n","      # Display the progress\n","      if (i+1) % 300 == 0:\n","        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","              .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","        \n","# Test the model\n","def test_logreg(model, test_loader):\n","  # In test phase, we don't need to compute gradients (for memory efficiency)\n","  with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","      images = images.reshape(-1, 28*28)\n","      outputs = model(images)\n","      _, predicted = torch.max(outputs.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum()\n","\n","    # Display the result\n","    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ZUjFlmQlqiC"},"source":["Now let us use the above script to train and evaluate the logistic regression model.\n","\n","**Q: How much accuracy could you obtain?**"]},{"cell_type":"code","metadata":{"id":"uxPKOiXklp5z"},"source":["train_logreg(train_loader, num_epochs=10)\n","test_logreg(model, test_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ziQdrhumDz7"},"source":["## 9.3. Feed-forward Neural Networks\n","\n","This section shows how to define and apply a feed-forward neural network (or multi-layer perceptron, MLP) to the MNIST dataset.\n"]},{"cell_type":"markdown","metadata":{"id":"7BNLh-Q2mJTO"},"source":["First, let us specify the hyperparameters for model and training."]},{"cell_type":"code","metadata":{"id":"jh96hYAlmkqO"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters \n","hidden_size = 500\n","learning_rate = 0.001"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"evs47OuDmlNL"},"source":["Next code defines the class for a feed-forward neural network."]},{"cell_type":"code","metadata":{"id":"4Ih-Y1DkmxK_"},"source":["# Fully connected neural network with one hidden layer\n","class FFNet(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(FFNet, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size) \n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size, num_classes)  \n","    \n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BCea2Bscm0CG"},"source":["Below defines training and prediction with the above defined model."]},{"cell_type":"code","metadata":{"id":"dONo5NoHm7qh"},"source":["# Train the model\n","def train_ffnet(model, train_loader, num_epochs):\n","  total_step = len(train_loader)\n","  for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):  \n","      # Move tensors to the configured device\n","      \n","      images = images.reshape(-1, 28*28).to(device)\n","      labels = labels.to(device)\n","      \n","      # Forward pass\n","      outputs = model(images)\n","      loss = criterion(outputs, labels)\n","      \n","      # Backward and optimize\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      \n","      # Display the progress\n","      if (i+1) % 300 == 0:\n","        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n","               .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","        \n","# Test the model\n","def test_ffnet(model, test_loader):\n","  # In test phase, we don't need to compute gradients (for memory efficiency)\n","  with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","      images = images.reshape(-1, 28*28).to(device)\n","      labels = labels.to(device)\n","      outputs = model(images)\n","      _, predicted = torch.max(outputs.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","\n","    # Display the result\n","    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EtnzdTdBm-Mi"},"source":["Now let us run the script on the MNIST dataset.\n","\n","**Q: What is the accuracy that a feed-forward NN can achieve? Is it better than what you got from a logistic regression?**"]},{"cell_type":"markdown","metadata":{"id":"rSDZ4_ZtnadD"},"source":["## 9.4. Convolutional Neural Networks\n","\n","This section deals with convolutional neural network (CNN), that is commonly used for image classification.\n","\n","First off, let us declare the hyperparameters for training."]},{"cell_type":"code","metadata":{"id":"2qQdycaTn2wa"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters \n","learning_rate = 0.001"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MhsbwlEqoFVM"},"source":["Next code defines a convolutional neural network used in this tutorial."]},{"cell_type":"code","metadata":{"id":"Hk2L3_3LoL_B"},"source":["# Convolutional neural network (two convolutional layers)\n","class ConvNet(nn.Module):\n","  def __init__(self, num_classes=10):\n","    super(ConvNet, self).__init__()\n","    self.layer1 = nn.Sequential(\n","      nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n","      nn.BatchNorm2d(16),\n","      nn.ReLU(),\n","      nn.MaxPool2d(kernel_size=2, stride=2))\n","    self.layer2 = nn.Sequential(\n","      nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n","      nn.BatchNorm2d(32),\n","      nn.ReLU(),\n","      nn.MaxPool2d(kernel_size=2, stride=2))\n","    self.fc = nn.Linear(7*7*32, num_classes)\n","      \n","  def forward(self, x):\n","    out = self.layer1(x)\n","    out = self.layer2(out)\n","    out = out.reshape(out.size(0), -1)\n","    out = self.fc(out)\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pPfSnaAHoLUw"},"source":["Below defines the training and prediction process."]},{"cell_type":"code","metadata":{"id":"aPhvacuMo7vu"},"source":["# Train the model\n","def train_convnet(model, train_loader, num_epochs):\n","  total_step = len(train_loader)\n","  for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      \n","      # Forward pass\n","      outputs = model(images)\n","      loss = criterion(outputs, labels)\n","      \n","      # Backward and optimize\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      \n","      # Display the progress\n","      if (i+1) % 300 == 0:\n","        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","              .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","\n","# Test the model      \n","def test_convnet(model, test_loader):\n","  model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n","  with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      outputs = model(images)\n","      _, predicted = torch.max(outputs.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","\n","    # Display the result\n","    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G1IztZaSpBYP"},"source":["Let us run and apply the above code to the MNIST dataset.\n","\n","**Q: How the CNN model performs? Is the accuracy better than that of logistic regression and feed-forward neural network?**"]},{"cell_type":"code","metadata":{"id":"hj-iBbQgpDgI"},"source":["model = ConvNet(num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_convnet(model, train_loader, num_epochs=10)\n","test_convnet(model, test_loader)"],"execution_count":null,"outputs":[]}]}